# Let's Calculate the Costs for Using ChatGPT Plus vs API

As artificial intelligence continues to advance, more and more companies are incorporating AI-powered chatbots into their customer service systems. These chatbots can handle a wide range of customer inquiries, from simple questions to more complex issues. However, the cost of implementing and maintaining such chatbots is an important factor to consider. In this article, we will calculate the costs of using GPT-4 and GPT-3.5-turbo models with a message cap of 25 messages every 3 hours for one month of usage, considering the same average prompt sizes (50 to 200 tokens).

Understanding the Message Cap and Average Prompt Sizes
Before we dive into the costs, it's important to understand the message cap and average prompt sizes. The message cap refers to the maximum number of messages that can be sent in a given time period, in this case, 25 messages every 3 hours for one month. The average prompt size is the length of the text input that is fed into the chatbot. In this article, we will be considering prompt sizes ranging from 50 to 200 tokens.

Calculating the Costs
To calculate the costs of using GPT-4 and GPT-3.5-turbo models with a message cap of 25 messages every 3 hours for one month of usage, we will use the following formula:

* 1k tokens is equivalent to ~750 words * 

Cost = (Number of messages * Prompt size * Price per token) / 1000

We will calculate the costs for both the lower bound (50 tokens per message) and the upper bound (200 tokens per message) of the average prompt size.

Lower Bound (50 tokens per message)
Using the formula above, we can calculate the costs for the lower bound of the average prompt size as follows:

GPT-4 8K context cost = (6,000 * 50 * ($0.03 + $0.06)) / 1000 = $27
GPT-4 32K context cost = (6,000 * 50 * ($0.06 + $0.12)) / 1000 = $54
GPT-3.5-turbo cost = (6,000 * 50 * $0.002) / 1000 = $0.6
Upper Bound (200 tokens per message)
Similarly, we can calculate the costs for the upper bound of the average prompt size as follows:

GPT-4 8K context cost = (6,000 * 200 * ($0.03 + $0.06)) / 1000 = $108
GPT-4 32K context cost = (6,000 * 200 * ($0.06 + $0.12)) / 1000 = $216
GPT-3.5-turbo cost = (6,000 * 200 * $0.002) / 1000 = $2.4
The GPT-4 API is a more advanced and costly option compared to the ChatGPT API. The GPT-4 API has doubled the max context length of tokens from GPT-3.5 and has default rate limits of 40k tokens per minute and 200 requests per minute. The GPT-4 API costs $0.03 per 1k prompt request tokens and $0.06 per 1k completion response tokens. In comparison, the ChatGPT API is 14x less expensive for prompts and 29x less expensive for completions. Choosing between the two APIs depends on specific project requirements and considerations, including intended application, accuracy, ethical considerations, financial implications, and adaptability to future advancements. OpenAI also provides limited access to the 32,768 context version of the GPT-4 API at 2x the cost of the 8,092 max context length version.

The Price of Plugin Features and the Value of GPT-4
Align to the left
Align in the middle
Resize to full width
Align to the right
Add a link to the embedded image

Add alt text

No alt text provided for this image
https://twitter.com/emollick/status/1635700173946105856/photo/1
When it comes to choosing the right AI-powered chatbot for your business, one of the most important factors to consider is the cost. Different chatbots come with different pricing structures, depending on the features and capabilities they offer. In this article, we will discuss the price of plugin features and the value of GPT-4.

Limitations of ChatGPT
ChatGPT, like any other large language model, has its limitations. OpenAI has acknowledged that ChatGPT may produce plausible-sounding but incorrect or nonsensical answers, a behavior commonly known as "hallucination". This can be attributed to the reward model designed around human oversight, which may cause over-optimization and hinder performance, a phenomenon referred to as Goodhart's law.

Another limitation of ChatGPT is its limited knowledge of events that occurred after September 2021. This means that ChatGPT may not be able to provide accurate information about recent events or updates.

Human reviewers' preferences for longer answers during ChatGPT training, irrespective of factual content or comprehension, is another limitation. Additionally, ChatGPT's training data is subject to algorithmic bias, which may be evident when it responds to prompts including descriptors of people. In one instance, ChatGPT generated a rap that indicated inferiority of women and scientists of color to white and male scientists, revealing the presence of bias.

Understanding Plugin Features
A plugin is a software component that adds specific functionality to an existing application or software platform. In the context of chatbots, plugins can add new features and capabilities to enhance the user experience. Some common chatbot plugins include:

Integration with third-party services (e.g., payment gateways, CRM systems)
Natural Language Processing (NLP) plugins for better understanding and responding to user queries
Customized chatbot workflows for specific business processes
Multilingual support for global businesses
Onboarding new 
Zapier is a web-based service that connects different apps and automates workflows. It allows users to create automated tasks, or "Zaps," that trigger actions in one app based on events in another app. In the context of chatbots, Zapier can be used to integrate a chatbot with other third-party services, such as payment gateways, CRMs, and social media platforms.

Web access



Alternative
Conclusion
In conclusion, the estimated costs of using GPT-4 and GPT-3.5-turbo models with a message cap of 25 messages every 3 hours for one month of usage, considering the same average prompt sizes (50 to 200 tokens), are as follows:

GPT-4 8K context: $27 to $108 (depending on the average prompt size)
GPT-4 32K context: $54 to $216 (depending on the average prompt size)
GPT-3.5-turbo: $0.6 to $2.4


Source
